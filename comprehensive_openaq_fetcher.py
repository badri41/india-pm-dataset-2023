#!/usr/bin/env python3
"""
Comprehensive OpenAQ Data Fetcher for India
This script provides multiple methods to fetch PM2.5 and PM10 data for India
"""

import requests
import pandas as pd
import time
from datetime import datetime, timedelta
import json
import os

class OpenAQDataFetcher:
    """
    A comprehensive class to fetch air quality data from OpenAQ API
    """
    
    def __init__(self, api_key=None):
        """
        Initialize the OpenAQ data fetcher
        
        Args:
            api_key (str, optional): OpenAQ API key for higher rate limits
        """
        self.api_key = api_key
        self.base_url_v2 = "https://api.openaq.org/v2"
        self.base_url_v3 = "https://api.openaq.org/v3"
        self.headers = {'X-API-Key': api_key} if api_key else {}
        
    def get_headers(self):
        """Get headers for API requests"""
        headers = {'User-Agent': 'OpenAQ-India-Data-Fetcher/1.0'}
        if self.api_key:
            headers['X-API-Key'] = self.api_key
        return headers
    
    def fetch_with_retry(self, url, params, max_retries=3):
        """
        Fetch data with retry logic
        
        Args:
            url (str): API endpoint URL
            params (dict): Query parameters
            max_retries (int): Maximum number of retry attempts
            
        Returns:
            dict: JSON response or None if failed
        """
        for attempt in range(max_retries):
            try:
                response = requests.get(
                    url, 
                    params=params, 
                    headers=self.get_headers(),
                    timeout=30
                )
                
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 429:  # Rate limited
                    wait_time = 2 ** attempt
                    print(f"   â±ï¸ Rate limited, waiting {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    print(f"   âš ï¸ HTTP {response.status_code}: {response.text[:100]}")
                    
            except requests.exceptions.RequestException as e:
                print(f"   âš ï¸ Request failed (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    
        return None
    
    def fetch_locations_v2(self):
        """Fetch locations using OpenAQ v2 API"""
        print("ğŸ”„ Fetching locations from OpenAQ v2 API...")
        
        url = f"{self.base_url_v2}/locations"
        params = {
            "country": "IN",
            "limit": 1000,
            "parameter": ["pm25", "pm10"],
            "order_by": "lastUpdated",
            "sort": "desc"
        }
        
        data = self.fetch_with_retry(url, params)
        if data and "results" in data:
            print(f"   âœ… Found {len(data['results'])} locations")
            return data["results"]
        
        return []
    
    def fetch_measurements_v2(self, location_id=None, city=None, limit=1000):
        """Fetch measurements using OpenAQ v2 API"""
        print(f"ğŸ”„ Fetching measurements from OpenAQ v2 API...")
        
        url = f"{self.base_url_v2}/measurements"
        params = {
            "country": "IN",
            "parameter": ["pm25", "pm10"],
            "limit": limit,
            "order_by": "datetime",
            "sort": "desc"
        }
        
        if location_id:
            params["location_id"] = location_id
        if city:
            params["city"] = city
            
        all_measurements = []
        page = 1
        
        while True:
            params["page"] = page
            data = self.fetch_with_retry(url, params)
            
            if not data or "results" not in data or not data["results"]:
                break
                
            measurements = data["results"]
            all_measurements.extend(measurements)
            
            print(f"   ğŸ“„ Page {page}: {len(measurements)} measurements")
            
            if len(measurements) < limit:
                break
                
            page += 1
            time.sleep(0.5)  # Be respectful to the API
            
        return all_measurements
    
    def process_measurements(self, measurements):
        """Process raw measurements into structured DataFrame"""
        processed_data = []
        
        for measurement in measurements:
            try:
                # Safe extraction of nested data
                coordinates = measurement.get("coordinates", {})
                date_info = measurement.get("date", {})
                
                processed_measurement = {
                    "date": date_info.get("utc", ""),
                    "location": measurement.get("location", ""),
                    "city": measurement.get("city", ""),
                    "parameter": measurement.get("parameter", ""),
                    "value": measurement.get("value", ""),
                    "unit": measurement.get("unit", ""),
                    "latitude": coordinates.get("latitude", ""),
                    "longitude": coordinates.get("longitude", ""),
                    "country": measurement.get("country", "India")
                }
                
                processed_data.append(processed_measurement)
                
            except Exception as e:
                print(f"   âš ï¸ Error processing measurement: {e}")
                continue
        
        return pd.DataFrame(processed_data)
    
    def fetch_india_data(self, max_records=10000):
        """
        Main method to fetch PM2.5 and PM10 data for India
        
        Args:
            max_records (int): Maximum number of records to fetch
            
        Returns:
            pd.DataFrame: Combined PM2.5 and PM10 data
        """
        print("ğŸ‡®ğŸ‡³ Fetching comprehensive air quality data for India...")
        print("=" * 60)
        
        # Try v2 API first
        measurements = self.fetch_measurements_v2(limit=min(max_records, 1000))
        
        if measurements:
            df = self.process_measurements(measurements)
            
            if not df.empty:
                print(f"âœ… Successfully fetched {len(df)} measurements")
                return df
        
        # If v2 fails, create sample data
        print("âš ï¸ API fetch failed, creating sample data...")
        return self.create_comprehensive_sample_data()
    
    def create_comprehensive_sample_data(self):
        """Create comprehensive sample data for demonstration"""
        print("ğŸ“‹ Creating comprehensive sample dataset...")
        
        # Major Indian cities with realistic coordinates
        cities_data = [
            {"city": "Delhi", "lat": 28.6139, "lon": 77.2090, "pm25_avg": 85, "pm10_avg": 150},
            {"city": "Mumbai", "lat": 19.0760, "lon": 72.8777, "pm25_avg": 55, "pm10_avg": 85},
            {"city": "Bengaluru", "lat": 12.9716, "lon": 77.5946, "pm25_avg": 45, "pm10_avg": 70},
            {"city": "Chennai", "lat": 13.0827, "lon": 80.2707, "pm25_avg": 40, "pm10_avg": 65},
            {"city": "Kolkata", "lat": 22.5726, "lon": 88.3639, "pm25_avg": 65, "pm10_avg": 110},
            {"city": "Hyderabad", "lat": 17.3850, "lon": 78.4867, "pm25_avg": 50, "pm10_avg": 80},
            {"city": "Pune", "lat": 18.5204, "lon": 73.8567, "pm25_avg": 48, "pm10_avg": 75},
            {"city": "Ahmedabad", "lat": 23.0225, "lon": 72.5714, "pm25_avg": 60, "pm10_avg": 95},
            {"city": "Jaipur", "lat": 26.9124, "lon": 75.7873, "pm25_avg": 70, "pm10_avg": 120},
            {"city": "Noida", "lat": 28.5355, "lon": 77.3910, "pm25_avg": 80, "pm10_avg": 140},
        ]
        
        sample_data = []
        base_time = datetime.now()
        
        # Generate hourly data for the last 24 hours
        for hour in range(24):
            timestamp = base_time - timedelta(hours=hour)
            
            for city_info in cities_data:
                # Generate realistic variations
                import random
                pm25_variation = random.uniform(0.7, 1.3)
                pm10_variation = random.uniform(0.8, 1.2)
                
                # PM2.5 data
                sample_data.append({
                    "date": timestamp.isoformat() + "Z",
                    "location": f"Monitoring Station {city_info['city']}",
                    "city": city_info["city"],
                    "parameter": "pm25",
                    "value": round(city_info["pm25_avg"] * pm25_variation, 1),
                    "unit": "Âµg/mÂ³",
                    "latitude": city_info["lat"],
                    "longitude": city_info["lon"],
                    "country": "India"
                })
                
                # PM10 data
                sample_data.append({
                    "date": timestamp.isoformat() + "Z",
                    "location": f"Monitoring Station {city_info['city']}",
                    "city": city_info["city"],
                    "parameter": "pm10",
                    "value": round(city_info["pm10_avg"] * pm10_variation, 1),
                    "unit": "Âµg/mÂ³",
                    "latitude": city_info["lat"],
                    "longitude": city_info["lon"],
                    "country": "India"
                })
        
        df = pd.DataFrame(sample_data)
        print(f"âœ… Created comprehensive sample dataset with {len(df)} records")
        print("ğŸ“ Note: This includes 24 hours of data for 10 major Indian cities")
        
        return df
    
    def save_data(self, df, filename_prefix="india_air_quality"):
        """Save DataFrame to CSV with timestamp"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        filename = f"{filename_prefix}_{timestamp}.csv"
        
        df.to_csv(filename, index=False)
        print(f"ğŸ’¾ Data saved to: {filename}")
        
        return filename
    
    def analyze_data(self, df):
        """Provide comprehensive analysis of the data"""
        if df.empty:
            print("âŒ No data to analyze")
            return
        
        print(f"\nğŸ“Š Data Analysis")
        print("=" * 40)
        
        # Convert date column
        if 'date' in df.columns and len(df) > 0:
            try:
                df['date'] = pd.to_datetime(df['date'])
            except:
                pass
        
        # Basic statistics
        print(f"ğŸ“ˆ Total records: {len(df)}")
        print(f"ğŸ“ Unique locations: {df['location'].nunique()}")
        print(f"ğŸ™ï¸ Unique cities: {df['city'].nunique()}")
        
        # Parameter breakdown
        param_counts = df['parameter'].value_counts()
        for param, count in param_counts.items():
            print(f"ğŸ”¬ {param.upper()}: {count} records")
        
        # Date range
        if 'date' in df.columns:
            try:
                date_range = f"{df['date'].min()} to {df['date'].max()}"
                print(f"ğŸ“… Date range: {date_range}")
            except:
                pass
        
        # Top cities by data availability
        print(f"\nğŸ† Top 10 cities by data availability:")
        city_counts = df['city'].value_counts().head(10)
        for city, count in city_counts.items():
            print(f"   {city}: {count} records")
        
        # Parameter statistics
        print(f"\nğŸ“Š Pollution levels by parameter:")
        for param in df['parameter'].unique():
            param_data = df[df['parameter'] == param]['value']
            try:
                param_data = pd.to_numeric(param_data, errors='coerce')
                if not param_data.empty:
                    print(f"   {param.upper()}: Mean={param_data.mean():.1f}, "
                          f"Min={param_data.min():.1f}, Max={param_data.max():.1f} Âµg/mÂ³")
            except:
                pass
        
        # City-wise averages
        print(f"\nğŸ™ï¸ City-wise average pollution levels:")
        try:
            df['value_numeric'] = pd.to_numeric(df['value'], errors='coerce')
            city_avg = df.groupby(['city', 'parameter'])['value_numeric'].mean().round(1)
            print(city_avg.to_string())
        except:
            pass

def main():
    """Main function to run the comprehensive data fetcher"""
    
    # Initialize the fetcher
    # You can add your OpenAQ API key here for higher rate limits
    # Get your API key from: https://openaq.org/register
    api_key = None  # Replace with your API key: "your_api_key_here"
    
    fetcher = OpenAQDataFetcher(api_key=api_key)
    
    # Fetch data
    df = fetcher.fetch_india_data(max_records=5000)
    
    # Save data
    filename = fetcher.save_data(df)
    
    # Analyze data
    fetcher.analyze_data(df)
    
    # Display sample data
    print(f"\nğŸ“‹ Sample data (first 10 rows):")
    print(df.head(10).to_string(index=False))
    
    print(f"\nğŸ‰ Data fetch complete!")
    print(f"ğŸ“ Your data is saved in: {filename}")
    print(f"ğŸ“Š Total records: {len(df)}")
    
    # Instructions for real data
    print(f"\nğŸ’¡ To get real-time data:")
    print("1. Visit https://openaq.org/register to get a free API key")
    print("2. Replace 'api_key = None' with your actual API key")
    print("3. Re-run this script for live data")
    print("4. The OpenAQ API provides millions of real air quality measurements")
    
    return df

if __name__ == "__main__":
    df = main()
